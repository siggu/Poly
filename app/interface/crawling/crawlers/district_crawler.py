"""
ì›Œí¬í”Œë¡œìš°: ë§í¬ ìˆ˜ì§‘ â†’ í¬ë¡¤ë§ ë° êµ¬ì¡°í™” (íƒ­ ì²˜ë¦¬ í¬í•¨ - ì»¨í…Œì´ë„ˆ í˜ì´ì§€ ì €ì¥)

1. ì´ˆê¸° ë§í¬ ìˆ˜ì§‘: ë³´ê±´ì†Œ ì‚¬ì´íŠ¸ì˜ LNB ë“±ì—ì„œ ì„œë¸Œ ë©”ë‰´ ë§í¬ ìˆ˜ì§‘
2. ë§í¬ ì²˜ë¦¬ ë£¨í”„:
   - ê° ë§í¬ í˜ì´ì§€ ë°©ë¬¸
   - í˜ì´ì§€ ë‚´ë¶€ì— íƒ­ ë©”ë‰´ê°€ ìˆëŠ”ì§€ í™•ì¸
   - â˜…â˜…â˜… í˜„ì¬ í˜ì´ì§€ ë‚´ìš©ì„ LLMìœ¼ë¡œ êµ¬ì¡°í™” (íƒ­ ìœ ë¬´ì™€ ìƒê´€ì—†ì´ í•­ìƒ ì‹¤í–‰) â˜…â˜…â˜…
   - íƒ­ ë°œê²¬ ì‹œ: ìƒˆë¡œìš´ íƒ­ ë§í¬ë“¤ì„ ì²˜ë¦¬ ëª©ë¡ì— ì¶”ê°€
3. ëª¨ë“  ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
"""

import json
import os
from datetime import datetime
from typing import List, Dict, Set
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin
import time

# ê³µí†µ ëª¨ë“ˆ import
import sys

sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))
import config
import utils
from base.base_crawler import BaseCrawler
from base.llm_crawler import LLMStructuredCrawler


class HealthCareWorkflow(BaseCrawler):
    """ë³´ê±´ì†Œ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ë° êµ¬ì¡°í™” ì›Œí¬í”Œë¡œìš° (íƒ­ ì²˜ë¦¬ ê¸°ëŠ¥ í¬í•¨ - ì»¨í…Œì´ë„ˆ ì €ì¥)"""

    def __init__(self, output_dir: str = "app/interface/crawling/output", region: str = None):
        """
        Args:
            output_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
            region: ì§€ì—­ëª… (ì˜ˆ: "ë™ì‘êµ¬"). Noneì´ë©´ URLì—ì„œ ìë™ ì¶”ì¶œ ì‹œë„
        """
        super().__init__()  # BaseCrawler ì´ˆê¸°í™”
        self.output_dir = output_dir
        self.region = region
        # LLM í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì‹œ ëª¨ë¸ ì§€ì •
        self.crawler = LLMStructuredCrawler(model="gpt-4o-mini")  # ë˜ëŠ” "gpt-4o" ë“±

        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(output_dir, exist_ok=True)

    def collect_links(self, start_url: str, crawl_rules: List[Dict]) -> List[Dict]:
        """
        ì´ˆê¸° ë§í¬ ëª©ë¡ ìˆ˜ì§‘ (LNB ë“± - ì´ì „ê³¼ ë™ì¼í•œ ë¡œì§)
        """
        base_url = utils.get_base_url(start_url)

        # ì‚¬ì´íŠ¸ë³„ íŠ¹ìˆ˜ ì²˜ë¦¬ (ì¿ í‚¤, SSL) - BaseCrawlerì—ì„œ ì²˜ë¦¬
        verify_ssl = self._apply_site_specific_config(start_url)

        try:
            response = self.session.get(
                start_url, timeout=config.DEFAULT_TIMEOUT, verify=verify_ssl
            )
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "html.parser")
        except requests.RequestException as e:
            print(f"ì˜¤ë¥˜: ì‹œì‘ URL({start_url})ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
            return []

        # ì ìš©í•  ê·œì¹™ ì°¾ê¸°
        main_links_elements = []
        active_rule = None
        for rule in crawl_rules:
            if "domain" in rule and rule["domain"].lower() not in start_url.lower():
                continue

            # single_page ëª¨ë“œ ì²˜ë¦¬
            if rule.get("single_page", False):
                menu_container = soup.select_one(rule.get("menu_container", "body"))
                if not menu_container:
                    continue

                found_menu_scope = menu_container  # ê¸°ë³¸ íƒìƒ‰ ë²”ìœ„
                if filter_menu := rule.get("filter_menu"):
                    # íŠ¹ì • ë©”ë‰´ í•„í„°ë§ ë¡œì§ ê°•í™”
                    potential_parents = menu_container.select(
                        "li:has(> a)"
                    )  # aë¥¼ ì§ì ‘ ê°€ì§„ li íƒìƒ‰
                    matched_parent = None
                    for item in potential_parents:
                        link = item.find("a", recursive=False)
                        if link and filter_menu in link.get_text(strip=True):
                            matched_parent = item  # í•„í„°ë§ëœ ë©”ë‰´ì˜ li ì°¾ìŒ
                            break
                    if matched_parent:
                        found_menu_scope = (
                            matched_parent  # íƒìƒ‰ ë²”ìœ„ë¥¼ í•„í„°ë§ëœ ë©”ë‰´ë¡œ ì¢í˜
                        )
                    else:
                        print(
                            f"  ê²½ê³ : single_page ê·œì¹™ '{rule['name']}'ì—ì„œ filter_menu '{filter_menu}'ë¥¼ ì°¾ì§€ ëª»í•¨. ì „ì²´ ì»¨í…Œì´ë„ˆ íƒìƒ‰."
                        )
                        # ëª» ì°¾ìœ¼ë©´ ì „ì²´ ì»¨í…Œì´ë„ˆì—ì„œ ê³„ì† ì§„í–‰ (ë˜ëŠ” continueë¡œ ë‹¤ìŒ ê·œì¹™ ì‹œë„)

                # ê²°ì •ëœ ë²”ìœ„(found_menu_scope) ë‚´ì—ì„œ main_selectorë¡œ ë§í¬ íƒìƒ‰
                main_links_elements = found_menu_scope.select(rule["main_selector"])

                if main_links_elements:
                    log_msg = f"'{rule['name']}' ({len(main_links_elements)}ê°œ ë§í¬ í›„ë³´ ë°œê²¬ - single_page"
                    if filter_menu:
                        log_msg += f", filter: '{filter_menu}'"
                    log_msg += ")"
                    print(f"  âœ“ ê·œì¹™ ì ìš©: {log_msg}")
                    active_rule = rule
                    break  # ê·œì¹™ ì°¾ìœ¼ë©´ ì¢…ë£Œ

            else:  # ì¼ë°˜ LNB ëª¨ë“œ
                main_links_elements = soup.select(rule["main_selector"])
                if main_links_elements:
                    print(
                        f"  âœ“ ê·œì¹™ ì ìš©: '{rule['name']}' ({len(main_links_elements)}ê°œ ë§í¬ ë°œê²¬)"
                    )
                    active_rule = rule
                    break

        if not active_rule:
            print(
                "ê²½ê³ : ì ìš© ê°€ëŠ¥í•œ í¬ë¡¤ë§ ê·œì¹™ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë¹ˆ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."
            )
            return []

        # ë§í¬ ì¶”ì¶œ ë° ë°˜í™˜
        collected_links = []
        seen_urls = set()

        # single_page ëª¨ë“œ ë§í¬ ì²˜ë¦¬
        if active_rule.get("single_page", False):
            # sub_selectorê°€ ìˆìœ¼ë©´ ê³„ì¸µ êµ¬ì¡°ë¡œ ì²˜ë¦¬
            if sub_selector := active_rule.get("sub_selector"):
                for depth1_element in main_links_elements:
                    # depth1_element ìì²´ê°€ ë§í¬ì¼ ìˆ˜ë„ ìˆê³ , ì•„ë‹ ìˆ˜ë„ ìˆìŒ. ìœ ì—°í•˜ê²Œ ì²˜ë¦¬
                    parent_element = (
                        depth1_element.find_parent("li") or depth1_element
                    )  # liê°€ ì—†ìœ¼ë©´ ìê¸° ìì‹ 
                    sub_link_elements = parent_element.select(sub_selector)

                    if (
                        not sub_link_elements
                    ):  # í•˜ìœ„ ë©”ë‰´ ì—†ìœ¼ë©´ 1depth ìì²´ê°€ ë§í¬ì¸ì§€ í™•ì¸
                        if depth1_element.name == "a":
                            sub_link_elements = [depth1_element]
                        else:  # a íƒœê·¸ê°€ ì•„ë‹ˆë©´ ê±´ë„ˆëœ€
                            continue

                    for link_element in sub_link_elements:
                        name = link_element.get_text(strip=True)
                        href = link_element.get("href", "")
                        if href and href != "#" and not href.startswith("javascript:"):
                            url = urljoin(base_url, href)
                            if url.startswith(base_url) and url not in seen_urls:
                                seen_urls.add(url)
                                collected_links.append({"name": name, "url": url})
            else:  # sub_selector ì—†ìœ¼ë©´ main_links_elementsê°€ ìµœì¢… ë§í¬
                for link_element in main_links_elements:
                    name = link_element.get_text(strip=True)
                    href = link_element.get("href", "")
                    if href and href != "#" and not href.startswith("javascript:"):
                        url = urljoin(base_url, href)
                        if url.startswith(base_url) and url not in seen_urls:
                            seen_urls.add(url)
                            collected_links.append({"name": name, "url": url})
            print(f"  âœ“ ì´ {len(collected_links)}ê°œ ë§í¬ ìˆ˜ì§‘ (single_page, ì¤‘ë³µ ì œê±°)")

        # ì¼ë°˜ LNB ëª¨ë“œ ë§í¬ ì²˜ë¦¬
        else:
            main_categories = []
            for link_element in main_links_elements:
                name = link_element.get_text(strip=True)
                href = link_element.get("href", "")
                if href and href != "#" and not href.startswith("javascript:"):
                    url = urljoin(base_url, href)
                    # urlì´ base_urlë¡œ ì‹œì‘í•˜ëŠ”ì§€ ë‹¤ì‹œ í•œë²ˆ í™•ì¸ (ì™¸ë¶€ ë§í¬ ë°©ì§€)
                    if url.startswith(base_url):
                        main_categories.append({"name": name, "url": url})
                    else:
                        print(f"    â†’ ì™¸ë¶€ ë§í¬ ê±´ë„ˆëœ€ (1ë‹¨ê³„): {url}")

            # ê° ì¹´í…Œê³ ë¦¬ ë°©ë¬¸í•˜ì—¬ í•˜ìœ„ ë©”ë‰´ ìˆ˜ì§‘
            for category in main_categories:
                # ì´ë¯¸ ì²˜ë¦¬ëœ URLì´ë©´ ê±´ë„ˆë›°ê¸° (ì¤‘ë³µ ë°©ì§€ ê°•í™”)
                if category["url"] in seen_urls:
                    print(f"\n  LNB í•˜ìœ„ íƒìƒ‰ ê±´ë„ˆëœ€ (ì´ë¯¸ ì²˜ë¦¬ë¨): {category['name']}")
                    continue

                print(f"\n  LNB í•˜ìœ„ íƒìƒ‰: {category['name']}")
                time.sleep(config.RATE_LIMIT_DELAY)  # Rate limiting

                try:
                    cat_response = self.session.get(
                        category["url"], timeout=10, verify=verify_ssl
                    )
                    cat_response.raise_for_status()
                    # ì¸ì½”ë”© ëª…ì‹œì  ì„¤ì • (í•„ìš”ì‹œ)
                    cat_response.encoding = (
                        cat_response.apparent_encoding
                        if cat_response.apparent_encoding
                        else "utf-8"
                    )
                    cat_soup = BeautifulSoup(cat_response.text, "html.parser")

                    sub_link_elements = []
                    sub_selectors = active_rule.get("sub_selector", [])
                    if isinstance(sub_selectors, str):
                        sub_selectors = [sub_selectors]

                    found_sub_links = False
                    if sub_selectors:  # sub_selectorê°€ ì •ì˜ëœ ê²½ìš°ì—ë§Œ íƒìƒ‰
                        for selector in sub_selectors:
                            elements = cat_soup.select(selector)
                            if elements:
                                sub_link_elements.extend(elements)
                                found_sub_links = True  # í•˜ë‚˜ë¼ë„ ì°¾ìœ¼ë©´ True

                    if found_sub_links:
                        print(f"    â†’ í•˜ìœ„ ë©”ë‰´ {len(sub_link_elements)}ê°œ ë°œê²¬")
                        for link_element in sub_link_elements:
                            name = link_element.get_text(strip=True)
                            href = link_element.get("href", "")
                            if (
                                href
                                and href != "#"
                                and not href.startswith("javascript:")
                            ):
                                url = urljoin(base_url, href)
                                if url.startswith(base_url) and url not in seen_urls:
                                    seen_urls.add(url)
                                    collected_links.append({"name": name, "url": url})
                    else:  # sub_selectorê°€ ì—†ê±°ë‚˜, ìˆì–´ë„ ëª» ì°¾ì€ ê²½ìš°
                        print(
                            "    â†’ í•˜ìœ„ ë©”ë‰´ ì—†ìŒ (ë˜ëŠ” sub_selector ì—†ìŒ), ì¹´í…Œê³ ë¦¬ ìì²´ ì¶”ê°€"
                        )
                        url = category["url"]
                        if url not in seen_urls:
                            seen_urls.add(url)
                            collected_links.append(
                                {"name": category["name"], "url": url}
                            )

                except requests.RequestException as e:
                    print(f"    âœ— ì˜¤ë¥˜: {category['url']} ë°©ë¬¸ ì‹¤íŒ¨ - {e}")
                except Exception as e:
                    print(f"    âœ— ì˜¤ë¥˜: {category['url']} ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ - {e}")

        # ìµœì¢… ë°˜í™˜ ì „ í•œë²ˆ ë” ì¤‘ë³µ ì œê±° (ì•ˆì „ ì¥ì¹˜)
        final_links = []
        final_seen_urls = set()
        for link in collected_links:
            if link["url"] not in final_seen_urls:
                final_links.append(link)
                final_seen_urls.add(link["url"])

        return final_links

    def run(
        self,
        start_url: str,
        crawl_rules: List[Dict] = None,
        save_links: bool = True,
    ) -> Dict:
        """
        ì „ì²´ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ (íƒ­ ì²˜ë¦¬ ë¡œì§ ìˆ˜ì •: ì»¨í…Œì´ë„ˆ í˜ì´ì§€ ì €ì¥)
        """
        print("=" * 80)
        print("ë³´ê±´ì†Œ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì›Œí¬í”Œë¡œìš° ì‹œì‘")
        print("=" * 80)

        # ê¸°ë³¸ í¬ë¡¤ë§ ê·œì¹™ (config.pyì—ì„œ ê°€ì ¸ì˜¤ê¸°)
        if crawl_rules is None:
            crawl_rules = config.CRAWL_RULES

        # 1ë‹¨ê³„: ì´ˆê¸° ë§í¬ ìˆ˜ì§‘
        print("\n[1ë‹¨ê³„] ì´ˆê¸° ë§í¬ ìˆ˜ì§‘ ì¤‘...")
        print("-" * 80)
        initial_links = self.collect_links(start_url, crawl_rules)
        print(f"\nâœ… ì´ {len(initial_links)}ê°œì˜ ì´ˆê¸° ë§í¬ ìˆ˜ì§‘ ì™„ë£Œ")

        if not initial_links:
            print("ì²˜ë¦¬í•  ë§í¬ê°€ ì—†ìŠµë‹ˆë‹¤. ì›Œí¬í”Œë¡œìš°ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return {}

        # ë§í¬ ì €ì¥ (ì´ˆê¸° ë§í¬ë§Œ)
        if save_links:
            links_file = os.path.join(self.output_dir, "collected_initial_links.json")
            try:
                with open(links_file, "w", encoding="utf-8") as f:
                    json.dump(initial_links, f, ensure_ascii=False, indent=2)
                print(f"ğŸ“„ ì´ˆê¸° ë§í¬ ëª©ë¡ ì €ì¥: {links_file}")
            except IOError as e:
                print(f"ê²½ê³ : ì´ˆê¸° ë§í¬ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨ - {e}")

        # 2ë‹¨ê³„: ë§í¬ ì²˜ë¦¬ ë£¨í”„ (íƒ­ ë§í¬ í¬í•¨)
        print("\n[2ë‹¨ê³„] í˜ì´ì§€ ì²˜ë¦¬ ë° LLM êµ¬ì¡°í™” (íƒ­ í¬í•¨)...")
        print("-" * 80)

        structured_data_list = []
        failed_urls = []
        links_to_process = list(initial_links)  # ì²˜ë¦¬í•  ë§í¬ ëª©ë¡ (í)
        processed_or_queued_urls: Set[str] = {
            link["url"] for link in initial_links
        }  # ì¤‘ë³µ ë°©ì§€ Set

        # íƒ­ ë©”ë‰´ë¥¼ ì°¾ëŠ” ë° ì‚¬ìš©í•  CSS ì„ íƒì ëª©ë¡ (config.pyì—ì„œ ê°€ì ¸ì˜¤ê¸°)
        tab_selectors = config.TAB_SELECTORS

        processed_count = 0

        # while ë£¨í”„ë¡œ ë³€ê²½í•˜ì—¬ ë™ì ìœ¼ë¡œ ì¶”ê°€ë˜ëŠ” íƒ­ ë§í¬ ì²˜ë¦¬
        while links_to_process:
            link_info = links_to_process.pop(0)  # íì—ì„œ ë§í¬ ê°€ì ¸ì˜¤ê¸°
            url = link_info["url"]
            name = link_info["name"]
            processed_count += 1
            total_links_estimate = len(processed_or_queued_urls)

            print(f"\n[{processed_count}/{total_links_estimate}*] ì²˜ë¦¬ ì‹œë„: {name}")
            print(f"  URL: {url}")
            time.sleep(1)  # ë¶€í•˜ ê°ì†Œ ì§€ì—°

            try:
                # 1. í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
                soup = self.crawler.fetch_page(url)
                if not soup:
                    raise ValueError("í˜ì´ì§€ ë‚´ìš©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

                # 2. íƒ­ ë©”ë‰´ í™•ì¸ (íƒ­ ì²˜ë¦¬ ë¡œì§ì€ ê·¸ëŒ€ë¡œ ìœ ì§€)
                found_tabs = False
                tab_links_on_page = []
                base_url = f"{urlparse(url).scheme}://{urlparse(url).netloc}"  # í˜„ì¬ í˜ì´ì§€ ê¸°ì¤€ base_url
                for tab_selector in tab_selectors:
                    tab_elements = soup.select(tab_selector)
                    if tab_elements:
                        for tab_link_element in tab_elements:
                            tab_name = tab_link_element.get_text(strip=True)
                            tab_href = tab_link_element.get("href", "")
                            if (
                                tab_href
                                and tab_href != "#"
                                and not tab_href.startswith("javascript:")
                            ):
                                tab_url = urljoin(base_url, tab_href)
                                # ìê¸° ìì‹ ì„ ê°€ë¦¬í‚¤ëŠ” íƒ­ ë§í¬ëŠ” ì œì™¸í•  í•„ìš” ì—†ìŒ (ì•„ë˜ ë¡œì§ì—ì„œ ê±¸ëŸ¬ì§)
                                if tab_url.startswith(base_url):
                                    tab_links_on_page.append(
                                        {"name": tab_name, "url": tab_url}
                                    )
                        if tab_links_on_page:
                            found_tabs = True
                            print(
                                f"    â†’ íƒ­ ë©”ë‰´ ë°œê²¬ ({len(tab_links_on_page)}ê°œ í•­ëª©, ì„ íƒì: '{tab_selector}')"
                            )
                            break

                # â˜…â˜…â˜… 3. í˜„ì¬ í˜ì´ì§€ LLM êµ¬ì¡°í™” (íƒ­ ìœ ë¬´ì™€ ìƒê´€ì—†ì´ ì‹¤í–‰) â˜…â˜…â˜…
                print("    â†’ ë‚´ìš© êµ¬ì¡°í™” ì§„í–‰...")
                region = self.region or utils.extract_region_from_url(url)
                # LLM í˜¸ì¶œ ì‹œ ìˆ˜ì§‘ëœ nameì„ titleë¡œ ëª…í™•íˆ ì „ë‹¬
                structured_data = self.crawler.crawl_and_structure(
                    url=url,  # crawler ë‚´ë¶€ì—ì„œ fetch ë˜ëŠ” soup ì²˜ë¦¬
                    region=region,
                    title=name,
                )
                # ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                structured_data_list.append(structured_data.model_dump())
                print("  âœ… ì„±ê³µ")  # ì¼ë‹¨ í˜„ì¬ í˜ì´ì§€ ì²˜ë¦¬ ì„±ê³µ ë¡œê·¸

                # 4. íƒ­ ë°œê²¬ ì‹œ, ìƒˆë¡œìš´ íƒ­ ë§í¬ë§Œ íì— ì¶”ê°€
                if found_tabs:
                    newly_added_count = 0
                    for tab_link_info in tab_links_on_page:
                        # â˜…â˜…â˜… í˜„ì¬ URLê³¼ ë‹¤ë¥¸ URLì´ê³ , ì•„ì§ íì— ì—†ê±°ë‚˜ ì²˜ë¦¬ëœ ì  ì—†ëŠ” URLë§Œ ì¶”ê°€ â˜…â˜…â˜…
                        if (
                            tab_link_info["url"] != url
                            and tab_link_info["url"] not in processed_or_queued_urls
                        ):
                            links_to_process.append(tab_link_info)
                            processed_or_queued_urls.add(
                                tab_link_info["url"]
                            )  # íì— ì¶”ê°€ë˜ì—ˆìŒì„ ê¸°ë¡
                            newly_added_count += 1
                            print(
                                f"      + íƒ­ ë§í¬ ì¶”ê°€: {tab_link_info['name']} ({tab_link_info['url']})"
                            )
                    if newly_added_count > 0:
                        print(
                            f"    â†’ ìƒˆë¡œìš´ íƒ­ ë§í¬ {newly_added_count}ê°œë¥¼ ì²˜ë¦¬ ëª©ë¡ì— ì¶”ê°€í–ˆìŠµë‹ˆë‹¤."
                        )
                    # else:
                    # ì´ë¯¸ ì¶”ê°€ëœ ë§í¬ì— ëŒ€í•œ ë¡œê·¸ëŠ” ë¶ˆí•„ìš”í•˜ë¯€ë¡œ ì œê±°

            except Exception as e:
                print(f"  âŒ ì‹¤íŒ¨: {e}")
                # ì‹¤íŒ¨ ì‹œ ìƒì„¸ ì •ë³´ ê¸°ë¡
                import traceback

                error_details = traceback.format_exc()
                failed_urls.append(
                    {
                        "url": url,
                        "name": name,
                        "error": str(e),
                        "details": error_details,
                    }
                )
                print(f"  ì˜¤ë¥˜ ìƒì„¸:\n{error_details}")  # ì½˜ì†”ì—ë„ ìƒì„¸ ì˜¤ë¥˜ ì¶œë ¥

        # 3ë‹¨ê³„: ê²°ê³¼ ì €ì¥
        print("\n[3ë‹¨ê³„] ê²°ê³¼ ì €ì¥ ì¤‘...")
        print("-" * 80)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        # ì§€ì—­ëª…ì€ ì´ˆê¸° URL ê¸°ì¤€ ë˜ëŠ” ì§€ì •ëœ ê°’ ì‚¬ìš©
        region_name = self.region or utils.extract_region_from_url(start_url)

        # ì „ì²´ êµ¬ì¡°í™” ë°ì´í„° ì €ì¥
        output_file = os.path.join(
            self.output_dir, f"structured_data_{region_name}.json"
        )
        try:
            with open(output_file, "w", encoding="utf-8") as f:
                json.dump(structured_data_list, f, ensure_ascii=False, indent=2)
            print(f"âœ… êµ¬ì¡°í™” ë°ì´í„° ì €ì¥: {output_file}")
        except IOError as e:
            print(f"ì˜¤ë¥˜: êµ¬ì¡°í™” ë°ì´í„° íŒŒì¼ ì €ì¥ ì‹¤íŒ¨ - {e}")

        # ì‹¤íŒ¨í•œ URL ì €ì¥
        failed_file = None  # ì´ˆê¸°í™”
        if failed_urls:
            failed_file = os.path.join(
                self.output_dir, f"failed_urls_{region_name}.json"
            )
            try:
                with open(failed_file, "w", encoding="utf-8") as f:
                    # ì‹¤íŒ¨ ì •ë³´ì— ìƒì„¸ ì˜¤ë¥˜(details) í¬í•¨í•˜ì—¬ ì €ì¥
                    json.dump(failed_urls, f, ensure_ascii=False, indent=2)
                print(f"âš ï¸  ì‹¤íŒ¨í•œ URL ì €ì¥: {failed_file}")
            except IOError as e:
                print(f"ê²½ê³ : ì‹¤íŒ¨í•œ URL íŒŒì¼ ì €ì¥ ì‹¤íŒ¨ - {e}")

        # ìš”ì•½ ì •ë³´
        final_successful_count = len(structured_data_list)
        final_failed_count = len(failed_urls)
        # ì´ ì²˜ë¦¬ ì‹œë„ íšŸìˆ˜ëŠ” processed_count ì‚¬ìš©
        summary = {
            "timestamp": timestamp,
            "region": region_name,
            "start_url": start_url,
            "initial_links_collected": len(initial_links),
            "total_urls_processed_or_failed": processed_count,
            "successful_structured": final_successful_count,
            "failed_processing": final_failed_count,
            "output_file": output_file,
            "failed_urls_file": failed_file,  # ì‹¤íŒ¨ íŒŒì¼ ê²½ë¡œ ì €ì¥
        }

        summary_file = os.path.join(self.output_dir, f"summary_{timestamp}.json")
        try:
            with open(summary_file, "w", encoding="utf-8") as f:
                json.dump(summary, f, ensure_ascii=False, indent=2)
            print(f"ğŸ“Š ìš”ì•½ ì •ë³´ ì €ì¥: {summary_file}")
        except IOError as e:
            print(f"ê²½ê³ : ìš”ì•½ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨ - {e}")

        # ìµœì¢… ìš”ì•½ ì¶œë ¥
        print("\n" + "=" * 80)
        print("ì›Œí¬í”Œë¡œìš° ì™„ë£Œ")
        print("=" * 80)
        print(f"ğŸ“Š ì´ˆê¸° ìˆ˜ì§‘ ë§í¬ ìˆ˜: {len(initial_links)}")
        print(f"ğŸ”„ ì´ ì²˜ë¦¬ ì‹œë„ URL ìˆ˜: {processed_count}")
        print(f"âœ… ì„±ê³µ (êµ¬ì¡°í™”): {final_successful_count}ê°œ")
        print(f"âŒ ì‹¤íŒ¨: {final_failed_count}ê°œ")
        print(f"ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {self.output_dir}")
        print("=" * 80)

        return summary


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (ì´ì „ê³¼ ë™ì¼)"""
    import argparse

    parser = argparse.ArgumentParser(
        description="ë³´ê±´ì†Œ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ë° êµ¬ì¡°í™” ì›Œí¬í”Œë¡œìš°"
    )
    parser.add_argument("--url", type=str, help="ì‹œì‘ URL (ë³´ê±´ì†Œ ë³´ê±´ì‚¬ì—… í˜ì´ì§€)")
    parser.add_argument(
        "--output-dir",
        type=str,
        default="app/interface/crawling/output",
        help="ê²°ê³¼ë¥¼ ì €ì¥í•  ê¸°ë³¸ ë””ë ‰í† ë¦¬. ìµœì¢… ê²½ë¡œëŠ” 'app/interface/crawling/output/ì§€ì—­ëª…' í˜•íƒœê°€ ë©ë‹ˆë‹¤.",
    )
    parser.add_argument(
        "--region",
        type=str,
        default=None,
        help="ì§€ì—­ëª… (ì˜ˆ: ë™ì‘êµ¬). ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ URLì—ì„œ ìë™ ì¶”ì¶œ",
    )

    args = parser.parse_args()

    url = args.url
    region = args.region

    # URL ì—†ìœ¼ë©´ ì…ë ¥ë°›ê¸°
    if not url:
        print("\n" + "=" * 80)
        print("ë³´ê±´ì†Œ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì›Œí¬í”Œë¡œìš°")
        print("=" * 80)
        url = input("\nì‹œì‘ URLì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
        if not url:
            print("âŒ URLì„ ì…ë ¥í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return

    # ì§€ì—­ëª… ê²°ì •
    region_name = region or utils.extract_region_from_url(url)
    if not region_name or region_name == "unknown":
        print(
            "ê²½ê³ : URLì—ì„œ ì§€ì—­ëª…ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ê±°ë‚˜ 'unknown'ì…ë‹ˆë‹¤. ê¸°ë³¸ ë””ë ‰í† ë¦¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."
        )
        region_name = "default_region"  # ë˜ëŠ” ë‹¤ë¥¸ ê¸°ë³¸ê°’ ì‚¬ìš©

    # ìµœì¢… ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    output_dir = os.path.join(args.output_dir, region_name)

    # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
    workflow = HealthCareWorkflow(
        output_dir=output_dir, region=region_name
    )  # region_name ì „ë‹¬

    try:
        summary = workflow.run(start_url=url)
        print("\nâœ… ì›Œí¬í”Œë¡œìš° ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ!")

    except Exception as e:
        print(f"\nâŒ ì›Œí¬í”Œë¡œìš° ì‹¤íŒ¨: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
